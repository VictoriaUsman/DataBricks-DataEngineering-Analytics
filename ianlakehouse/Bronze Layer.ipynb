{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf49fd95-8406-4650-b90f-dca02a9a098f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql SHOW EXTERNAL LOCATIONS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a984e00-253c-47ff-9421-0dcf568bc014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW STORAGE CREDENTIALS\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17312825-ebf6-409e-a948-196638cc2ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###CREATING VOLUME STORAGE FROM S3 BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d70534-8762-47e4-9e9f-e30ffb6918bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE EXTERNAL LOCATION `db_s3_external_databricks-s3-ingest-5cdc9`\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39242cb2-8a8c-48bf-b33b-2e3dfb359746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL VOLUME IF NOT EXISTS iandatabricks.default.crm_raw_files\n",
    "LOCATION 's3://databricks-ian/sourceCRM/'\n",
    "COMMENT 'Clean shortcut to my CRM files';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "910ba5f1-b180-4348-89d2-0599bda1db12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingesting to Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73fecfc9-1a79-4be4-a362-402fb846858f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ERP folder.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dac4c1b-2025-46eb-876e-083a0bda68ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "import time\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "ingestion_map = [\n",
    "    {\"file\": \"CUST_AZ12.csv\", \"table\": \"erp_bronze_customers\"},\n",
    "    {\"file\": \"LOC_A101.csv\", \"table\": \"erp_bronze_products\"},\n",
    "    {\"file\": \"PX_CAT_G1V2.csv\", \"table\": \"erp_bronze_transactions\"}\n",
    "]\n",
    "source_base = \"/Volumes/iandatabricks/default/erp_raw_files/\"\n",
    "\n",
    "# --- 2. Execution Loop ---\n",
    "for item in ingestion_map:\n",
    "    file_filter = item[\"file\"]\n",
    "    target = f\"iandatabricks.bronze.{item['table']}\"\n",
    "    checkpoint = f\"{source_base}_checkpoints/{item['table']}\"\n",
    "    \n",
    "    query = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"pathGlobFilter\", file_filter)\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint}/schema\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(source_base)\n",
    "        .select(\"*\", \"_metadata.file_path\")\n",
    "        .withColumn(\"load_timestamp\", current_timestamp())\n",
    "        .writeStream\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint}/data\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target))\n",
    "\n",
    "    # Wait for the batch to finish so we can read the stats\n",
    "    query.awaitTermination()\n",
    "    \n",
    "    # --- 3. The Duplicate Check Logic ---\n",
    "    # We look at the last progress report from the stream\n",
    "    recent_progress = query.recentProgress\n",
    "    if recent_progress:\n",
    "        # Sum up all input rows from the batch\n",
    "        rows_added = sum(p['numInputRows'] or 0 for p in recent_progress)\n",
    "        \n",
    "        if rows_added > 0:\n",
    "            print(f\"✅ SUCCESS: Added {rows_added} new rows to {target} from {file_filter}.\")\n",
    "        else:\n",
    "            print(f\"⚠️  SKIP: No data added to {target}. File {file_filter} is a duplicate or already processed.\")\n",
    "    else:\n",
    "        print(f\"ℹ️  No changes detected for {target}.\")\n",
    "\n",
    "print(\"\\n--- All ingestion tasks complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8a6edf-92c6-42e9-beff-4810a1cd2363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the table content and the new file_path column\n",
    "display(spark.table(\"iandatabricks.bronze.bronze_erp_customer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107ba94f-128b-4afd-8371-74dba8117a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select count(*) from iandatabricks.bronze.erp_bronze_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea5851e-c4b1-4c68-a3e0-479916ae98eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table iandatabricks.bronze.bronze_erp_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fbf4f75-0a11-4125-acdb-c47fb9caac74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CRM **Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f80b0f-6e13-44cd-8604-41167fa010c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run this to wipe the memory and start fresh\n",
    "dbutils.fs.rm(checkpoint_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64e60cc-83a4-45a2-933f-081c2a610a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "ingestion_map = [\n",
    "    {\"file\": \"cust_info.csv\", \"table\": \"crm_custinfo\"},\n",
    "    {\"file\": \"prd_info.csv\", \"table\": \"crm_prdinfo\"},\n",
    "    {\"file\": \"sales_details.csv\", \"table\": \"crm_salesdeets\"}\n",
    "]\n",
    "source_base = \"/Volumes/iandatabricks/default/crm_raw_files\"\n",
    "\n",
    "# --- 2. Execution Loop ---\n",
    "for item in ingestion_map:\n",
    "    file_filter = item[\"file\"]\n",
    "    target = f\"iandatabricks.bronze.{item['table']}\"\n",
    "    checkpoint = f\"{source_base}/_checkpoints/{item['table']}\"\n",
    "    \n",
    "    query = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"pathGlobFilter\", file_filter)\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint}/schema\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(source_base)\n",
    "        .select(\"*\", \"_metadata.file_path\")\n",
    "        .withColumn(\"load_timestamp\", current_timestamp())\n",
    "        .writeStream\n",
    "        .option(\"checkpointLocation\", f\"{checkpoint}/data\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target))\n",
    "\n",
    "    # Wait for the batch to finish so we can read the stats\n",
    "    query.awaitTermination()\n",
    "    \n",
    "    # --- 3. The Duplicate Check Logic ---\n",
    "    # We look at the last progress report from the stream\n",
    "    recent_progress = query.recentProgress\n",
    "    if recent_progress:\n",
    "        # Sum up all input rows from the batch\n",
    "        rows_added = sum(p['numInputRows'] or 0 for p in recent_progress)\n",
    "        \n",
    "        if rows_added > 0:\n",
    "            print(f\"✅ SUCCESS: Added {rows_added} new rows to {target} from {file_filter}.\")\n",
    "        else:\n",
    "            print(f\"⚠️  SKIP: No data added to {target}. File {file_filter} is a duplicate or already processed.\")\n",
    "    else:\n",
    "        print(f\"ℹ️  No changes detected for {target}.\")\n",
    "\n",
    "print(\"\\n--- All ingestion tasks complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef5828ec-a395-43fe-9801-362b5d508497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6436411551081361,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
